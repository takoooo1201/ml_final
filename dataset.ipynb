{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "132dca5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kagglehub\n",
      "  Downloading kagglehub-0.3.13-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: packaging in /home/whsung8451/miniconda3/envs/ml_final/lib/python3.10/site-packages (from kagglehub) (25.0)\n",
      "Collecting pyyaml (from kagglehub)\n",
      "  Downloading pyyaml-6.0.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\n",
      "Collecting requests (from kagglehub)\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tqdm (from kagglehub)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->kagglehub)\n",
      "  Downloading charset_normalizer-3.4.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (37 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->kagglehub)\n",
      "  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->kagglehub)\n",
      "  Downloading urllib3-2.6.2-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/whsung8451/miniconda3/envs/ml_final/lib/python3.10/site-packages (from requests->kagglehub) (2025.11.12)\n",
      "Downloading kagglehub-0.3.13-py3-none-any.whl (68 kB)\n",
      "Downloading pyyaml-6.0.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (770 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m770.3/770.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n",
      "Downloading idna-3.11-py3-none-any.whl (71 kB)\n",
      "Downloading urllib3-2.6.2-py3-none-any.whl (131 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: urllib3, tqdm, pyyaml, idna, charset_normalizer, requests, kagglehub\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [kagglehub]/7\u001b[0m [requests]\n",
      "\u001b[1A\u001b[2KSuccessfully installed charset_normalizer-3.4.4 idna-3.11 kagglehub-0.3.13 pyyaml-6.0.3 requests-2.32.5 tqdm-4.67.1 urllib3-2.6.2\n"
     ]
    }
   ],
   "source": [
    "!pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ab0e746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/briscdataset/brisc2025?dataset_version_number=6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250M/250M [00:19<00:00, 13.6MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/whsung8451/.cache/kagglehub/datasets/briscdataset/brisc2025/versions/6\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"briscdataset/brisc2025\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c26d0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created validation directory: /home/whsung8451/.cache/kagglehub/datasets/briscdataset/brisc2025/versions/6/brisc2025/classification_task/val\n",
      "Processing class 'no_tumor': Moving 213 of 1067 images to validation set.\n",
      "Processing class 'meningioma': Moving 265 of 1329 images to validation set.\n",
      "Processing class 'glioma': Moving 229 of 1147 images to validation set.\n",
      "Processing class 'pituitary': Moving 291 of 1457 images to validation set.\n",
      "Validation split completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# Define paths based on the user's provided location\n",
    "# Using expanduser to handle the \"~\" in the path\n",
    "base_path = Path(os.path.expanduser(\"~/.cache/kagglehub/datasets/briscdataset/brisc2025/versions/6/brisc2025/classification_task\"))\n",
    "train_dir = base_path / \"train\"\n",
    "val_dir = base_path / \"val\"\n",
    "\n",
    "# Validation split ratio (e.g., 0.2 for 20% validation data)\n",
    "val_ratio = 0.2\n",
    "\n",
    "# Set seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "def create_validation_split():\n",
    "    if not train_dir.exists():\n",
    "        print(f\"Error: Train directory not found at {train_dir}\")\n",
    "        return\n",
    "\n",
    "    # Check if val directory already exists and is not empty to prevent double splitting\n",
    "    if val_dir.exists() and any(val_dir.iterdir()):\n",
    "        print(f\"Validation directory {val_dir} already exists and is not empty. Skipping split.\")\n",
    "        return\n",
    "\n",
    "    # Create val directory\n",
    "    if not val_dir.exists():\n",
    "        val_dir.mkdir(parents=True)\n",
    "        print(f\"Created validation directory: {val_dir}\")\n",
    "\n",
    "    # Iterate over each class folder in train\n",
    "    class_folders = [d for d in train_dir.iterdir() if d.is_dir()]\n",
    "    \n",
    "    for class_folder in class_folders:\n",
    "        class_name = class_folder.name\n",
    "        \n",
    "        # Create corresponding class folder in val\n",
    "        val_class_dir = val_dir / class_name\n",
    "        if not val_class_dir.exists():\n",
    "            val_class_dir.mkdir(parents=True)\n",
    "        \n",
    "        # Get all files in the class folder\n",
    "        files = [f for f in class_folder.iterdir() if f.is_file()]\n",
    "        \n",
    "        # Shuffle files to ensure random split (avoiding data leakage from ordering)\n",
    "        random.shuffle(files)\n",
    "        \n",
    "        # Calculate number of validation samples\n",
    "        num_val = int(len(files) * val_ratio)\n",
    "        val_files = files[:num_val]\n",
    "        \n",
    "        print(f\"Processing class '{class_name}': Moving {num_val} of {len(files)} images to validation set.\")\n",
    "        \n",
    "        # Move files from train to val\n",
    "        for file in val_files:\n",
    "            shutil.move(str(file), str(val_class_dir / file.name))\n",
    "            \n",
    "    print(\"Validation split completed successfully.\")\n",
    "\n",
    "create_validation_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bcd0097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting images in ./classification_task...\n",
      "\n",
      "--- TRAIN SET ---\n",
      "  no_tumor: 854\n",
      "  meningioma: 1064\n",
      "  glioma: 918\n",
      "  pituitary: 1166\n",
      "Total train images: 4002\n",
      "\n",
      "--- VAL SET ---\n",
      "  no_tumor: 213\n",
      "  meningioma: 265\n",
      "  glioma: 229\n",
      "  pituitary: 291\n",
      "Total val images: 998\n",
      "\n",
      "--- TEST SET ---\n",
      "  no_tumor: 140\n",
      "  meningioma: 306\n",
      "  glioma: 254\n",
      "  pituitary: 300\n",
      "Total test images: 1000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the path to the dataset\n",
    "# Assuming './classification_task' based on your workspace structure\n",
    "dataset_path = './classification_task'\n",
    "splits = ['train', 'val', 'test']\n",
    "\n",
    "print(f\"Counting images in {dataset_path}...\")\n",
    "\n",
    "for split in splits:\n",
    "    split_path = os.path.join(dataset_path, split)\n",
    "    if os.path.exists(split_path):\n",
    "        total_images = 0\n",
    "        print(f\"\\n--- {split.upper()} SET ---\")\n",
    "        # Walk through the directory to count files in subdirectories (classes)\n",
    "        for root, dirs, files in os.walk(split_path):\n",
    "            # Skip the root folder itself, only count in subfolders if structure is split/class/image\n",
    "            if root == split_path:\n",
    "                continue\n",
    "                \n",
    "            class_name = os.path.basename(root)\n",
    "            count = len([f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff', '.webp'))])\n",
    "            if count > 0:\n",
    "                print(f\"  {class_name}: {count}\")\n",
    "                total_images += count\n",
    "        \n",
    "        print(f\"Total {split} images: {total_images}\")\n",
    "    else:\n",
    "        print(f\"Directory not found: {split_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
